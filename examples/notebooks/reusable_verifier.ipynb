{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusable Verifiers \n",
    "\n",
    "This notebook demonstrates how to create and reuse the same set of separated verifiers for different models. Specifically, we will use the same verifier for the following four models:\n",
    "\n",
    "- `1l_mlp sigmoid`\n",
    "- `1l_mlp relu`\n",
    "- `1l_conv sigmoid`\n",
    "- `1l_conv relu`\n",
    "\n",
    "When deploying EZKL verifiers on the blockchain, each associated model typically requires its own unique verifier, leading to increased on-chain state usage. \n",
    "However, with the reusable verifier, we can deploy a single verifier that can be used to verify proofs for any valid H2 circuit. This notebook shows how to do so. \n",
    "\n",
    "By reusing the same verifier across multiple models, we significantly reduce the amount of state bloat on the blockchain. Instead of deploying a unique verifier for each model, we deploy a unique and much smaller verifying key artifact (VKA) contract for each model while sharing a common separated verifier. The VKA contains the VK for the model as well circuit specific metadata that was otherwise hardcoded into the stack of the original non-reusable verifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "\n",
    "# Define the models\n",
    "class MLP_Sigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Sigmoid, self).__init__()\n",
    "        self.fc = nn.Linear(3, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class MLP_Relu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Relu, self).__init__()\n",
    "        self.fc = nn.Linear(3, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Conv_Sigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_Sigmoid, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=3, stride=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class Conv_Relu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_Relu, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=3, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the models\n",
    "mlp_sigmoid = MLP_Sigmoid()\n",
    "mlp_relu = MLP_Relu()\n",
    "conv_sigmoid = Conv_Sigmoid()\n",
    "conv_relu = Conv_Relu()\n",
    "\n",
    "# Dummy input tensor for mlp\n",
    "dummy_input_mlp = torch.tensor([[-1.5737053155899048, -1.708398461341858, 0.19544155895709991]])\n",
    "input_mlp_path = 'mlp_input.json'\n",
    "\n",
    "# Dummy input tensor for conv\n",
    "dummy_input_conv = torch.tensor([[[1.4124163389205933, 0.6938204169273376, 1.0664031505584717]]])\n",
    "input_conv_path = 'conv_input.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['mlp_sigmoid', 'mlp_relu', 'conv_sigmoid', 'conv_relu']\n",
    "models = [mlp_sigmoid, mlp_relu, conv_sigmoid, conv_relu]\n",
    "inputs = [dummy_input_mlp, dummy_input_mlp, dummy_input_conv, dummy_input_conv]\n",
    "input_paths = [input_mlp_path, input_mlp_path, input_conv_path, input_conv_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import ezkl\n",
    "\n",
    "for name, model, x, input_path in zip(names, models, inputs, input_paths):\n",
    "    # Create a new directory for the model if it doesn't exist\n",
    "    if not os.path.exists(name):\n",
    "        os.mkdir(name)\n",
    "    # Store the paths in each of their respective directories\n",
    "    model_path = os.path.join(name, \"network.onnx\")\n",
    "    compiled_model_path = os.path.join(name, \"network.compiled\")\n",
    "    pk_path = os.path.join(name, \"test.pk\")\n",
    "    vk_path = os.path.join(name, \"test.vk\")\n",
    "    settings_path = os.path.join(name, \"settings.json\")\n",
    "\n",
    "    witness_path = os.path.join(name, \"witness.json\")\n",
    "    sol_code_path = os.path.join(name, 'test.sol')\n",
    "    sol_key_code_path = os.path.join(name, 'test_key.sol')\n",
    "    abi_path = os.path.join(name, 'test.abi')\n",
    "    proof_path = os.path.join(name, \"proof.json\")\n",
    "\n",
    "    # Flips the neural net into inference mode\n",
    "    model.eval()\n",
    "\n",
    "    # Export the model\n",
    "    torch.onnx.export(model, x, model_path, export_params=True, opset_version=10,\n",
    "                      do_constant_folding=True, input_names=['input'],\n",
    "                      output_names=['output'], dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                                              'output': {0: 'batch_size'}})\n",
    "\n",
    "    data_array = ((x).detach().numpy()).reshape([-1]).tolist()\n",
    "    data = dict(input_data=[data_array])\n",
    "    json.dump(data, open(input_path, 'w'))\n",
    "\n",
    "    py_run_args = ezkl.PyRunArgs()\n",
    "    py_run_args.input_visibility = \"private\"\n",
    "    py_run_args.output_visibility = \"public\"\n",
    "    py_run_args.param_visibility = \"fixed\"  # private by default\n",
    "\n",
    "    res = ezkl.gen_settings(model_path, settings_path, py_run_args=py_run_args)\n",
    "    assert res == True\n",
    "\n",
    "    await ezkl.calibrate_settings(input_path, model_path, settings_path, \"resources\")\n",
    "\n",
    "    res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "    assert res == True\n",
    "\n",
    "    res = await ezkl.get_srs(settings_path)\n",
    "    assert res == True\n",
    "\n",
    "    # now generate the witness file\n",
    "    res = await ezkl.gen_witness(input_path, compiled_model_path, witness_path)\n",
    "    assert os.path.isfile(witness_path) == True\n",
    "\n",
    "    # SETUP \n",
    "    # We recommend disabling selector compression for the setup as it decreases the size of the VK artifact\n",
    "    res = ezkl.setup(compiled_model_path, vk_path, pk_path, disable_selector_compression=True)\n",
    "    assert res == True\n",
    "    assert os.path.isfile(vk_path)\n",
    "    assert os.path.isfile(pk_path)\n",
    "    assert os.path.isfile(settings_path)\n",
    "\n",
    "    # GENERATE A PROOF\n",
    "    res = ezkl.prove(witness_path, compiled_model_path, pk_path, proof_path, \"single\")\n",
    "    assert os.path.isfile(proof_path)\n",
    "\n",
    "    res = await ezkl.create_evm_verifier(vk_path, settings_path, sol_code_path, abi_path, reusable=True)\n",
    "    assert res == True\n",
    "\n",
    "    res = await ezkl.create_evm_vka(vk_path, settings_path, sol_key_code_path, abi_path)\n",
    "    assert res == True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# make sure anvil is running locally\n",
    "# $ anvil -p 3030\n",
    "\n",
    "RPC_URL = \"http://localhost:3030\"\n",
    "\n",
    "# Save process globally\n",
    "anvil_process = None\n",
    "\n",
    "def start_anvil():\n",
    "    global anvil_process\n",
    "    if anvil_process is None:\n",
    "        anvil_process = subprocess.Popen([\"anvil\", \"-p\", \"3030\", \"--code-size-limit=41943040\"])\n",
    "        if anvil_process.returncode is not None:\n",
    "            raise Exception(\"failed to start anvil process\")\n",
    "        time.sleep(3)\n",
    "\n",
    "def stop_anvil():\n",
    "    global anvil_process\n",
    "    if anvil_process is not None:\n",
    "        anvil_process.terminate()\n",
    "        anvil_process = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the generated verifiers are identical for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filecmp\n",
    "\n",
    "def compare_files(file1, file2):\n",
    "    return filecmp.cmp(file1, file2, shallow=False)\n",
    "\n",
    "sol_code_path_0 = os.path.join(\"mlp_sigmoid\", 'test.sol')\n",
    "sol_code_path_1 = os.path.join(\"mlp_relu\", 'test.sol')\n",
    "\n",
    "sol_code_path_2 = os.path.join(\"conv_sigmoid\", 'test.sol')\n",
    "sol_code_path_3 = os.path.join(\"conv_relu\", 'test.sol')\n",
    "\n",
    "\n",
    "assert compare_files(sol_code_path_0, sol_code_path_1) == True\n",
    "assert compare_files(sol_code_path_2, sol_code_path_3) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we deploy separate verifier that will be shared by the four models. We picked the `1l_mlp sigmoid` model as an example but you could have used any of the generated verifiers since they are all identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "addr_path_verifier = \"addr_verifier.txt\"\n",
    "sol_code_path = os.path.join(\"mlp_sigmoid\", 'test.sol')\n",
    "\n",
    "res = await ezkl.deploy_evm(\n",
    "    addr_path_verifier,\n",
    "    sol_code_path,\n",
    "    'http://127.0.0.1:3030',\n",
    "    \"verifier/reusable\"\n",
    ")\n",
    "\n",
    "assert res == True\n",
    "\n",
    "with open(addr_path_verifier, 'r') as file:\n",
    "    addr = file.read().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we deploy each of the unique VK-artifacts and verify them using the shared verifier deployed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    addr_path_vk = \"addr_vk.txt\"\n",
    "    sol_key_code_path = os.path.join(name, 'test_key.sol')\n",
    "    res = await ezkl.deploy_evm(addr_path_vk, sol_key_code_path, 'http://127.0.0.1:3030', \"vka\")\n",
    "    assert res == True\n",
    "\n",
    "    with open(addr_path_vk, 'r') as file:\n",
    "        addr_vk = file.read().rstrip()\n",
    "        \n",
    "    proof_path = os.path.join(name, \"proof.json\")\n",
    "    sol_code_path = os.path.join(name, 'vk.sol')\n",
    "    res = await ezkl.verify_evm(\n",
    "        addr,\n",
    "        proof_path,\n",
    "        \"http://127.0.0.1:3030\",\n",
    "        addr_vk = addr_vk\n",
    "    )\n",
    "    assert res == True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
