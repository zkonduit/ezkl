{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Cat and Dog classifier\n",
                "\n",
                "Here we showcase a full-end-to-end flow of:\n",
                "1. training a model for a specific task (classifying cats and dogs)\n",
                "2. creating a proof of classification\n",
                "3. creating and deploying an evm verifier\n",
                "4. verifying the proof of judgment using the verifier\n",
                "\n",
                "First we download a cat and dog related dataset.\n",
                "\n",
                "To download the dataset make sure you have the kaggle cli installed in your local env `pip install kaggle`. Make sure you set up your `kaggle.json` file as detailed [here](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication).\n",
                "Then run the associated `cat_and_dog_data.sh` data download script: `sh cat_and_dog_data.sh`.\n",
                "\n",
                "Make sure you set the `CATDOG_DATA_DIR` variables to point to the directory the `cat_and_dog_data.sh` script has downloaded to. This script also accepts an argument to download to a specific directory: `sh cat_and_dog_data.sh /path/to/cat/data`.\n",
                "\n",
                "This follows the tutorial drafted here: https://www.kaggle.com/code/tirendazacademy/cats-dogs-classification-with-pytorch/notebook\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import os\n",
                "# os.environ[\"CATDOG_DATA_DIR\"] = \"./data\"\n",
                "\n",
                "CATDOG_DATA_DIR = os.environ.get('CATDOG_DATA_DIR')\n",
                "\n",
                "#  if is none set to \"\"\n",
                "if CATDOG_DATA_DIR is None:\n",
                "    CATDOG_DATA_DIR = \"./data\"\n",
                "\n",
                "print(\"CATDOG_DATA_DIR: \", CATDOG_DATA_DIR)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CAT DOG Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# check if notebook is in colab\n",
                "try:\n",
                "    # install ezkl\n",
                "    import google.colab\n",
                "    import subprocess\n",
                "    import sys\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ezkl\"])\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
                "\n",
                "# rely on local installation of ezkl if the notebook is not in colab\n",
                "except:\n",
                "    pass\n",
                "\n",
                "import os\n",
                "\n",
                "\n",
                "def walk_through_dir(dir_path):\n",
                "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
                "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
                "\n",
                "image_path = CATDOG_DATA_DIR + \"/CATDOG\"\n",
                "walk_through_dir(image_path)\n",
                "\n",
                "\n",
                "\n",
                "train_dir = image_path + \"/training_set/training_set\"\n",
                "test_dir =  image_path + \"/test_set/test_set\"\n",
                "train_dir, test_dir\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dataset \n",
                "\n",
                "It is important to understand the dataset for deep learning analysis because the dataset is the foundation of any machine learning or deep learning model. A deep learning model can only be as good as the data it is trained on, and a poor understanding of the dataset can lead to poor model performance or even bias. Now let's take a look at an image in the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "from PIL import Image\n",
                "import glob\n",
                "from pathlib import Path\n",
                "\n",
                "# Set seed\n",
                "random.seed(42) \n",
                "\n",
                "# 1. Get all image paths (* means \"any combination\")\n",
                "image_path_list= glob.glob(f\"{image_path}/*/*/*/*.jpg\")\n",
                "\n",
                "# 2. Get random image path\n",
                "random_image_path = random.choice(image_path_list)\n",
                "\n",
                "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
                "image_class = Path(random_image_path).parent.stem\n",
                "\n",
                "# 4. Open image\n",
                "img = Image.open(random_image_path)\n",
                "\n",
                "# 5. Print metadata\n",
                "print(f\"Random image path: {random_image_path}\")\n",
                "print(f\"Image class: {image_class}\")\n",
                "print(f\"Image height: {img.height}\") \n",
                "print(f\"Image width: {img.width}\")\n",
                "img\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set_theme()\n",
                "\n",
                "# Turn the image into an array\n",
                "img_as_array = np.asarray(img)\n",
                "\n",
                "# Plot the image with matplotlib\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.imshow(img_as_array)\n",
                "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels]\")\n",
                "plt.axis(False);\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transforming Data\n",
                "\n",
                "Transforming data, also known as preprocessing, is an important step in deep learning analysis because it can help to improve the performance of the model and reduce the risk of bias. Let's play with the images a bit with the transform method"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, transforms\n",
                "\n",
                "IMAGE_WIDTH=128\n",
                "IMAGE_HEIGHT=128\n",
                "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
                "\n",
                "# Write transform for image\n",
                "data_transform = transforms.Compose([\n",
                "    # Resize the images to IMAGE_SIZE xIMAGE_SIZE \n",
                "    transforms.Resize(size=IMAGE_SIZE),\n",
                "    # Flip the images randomly on the horizontal\n",
                "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
                "    # Turn the image into a torch.Tensor\n",
                "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
                "])\n",
                "\n",
                "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
                "    random.seed(seed)\n",
                "    random_image_paths = random.sample(image_paths, k=n)\n",
                "    for image_path in random_image_paths:\n",
                "        with Image.open(image_path) as f:\n",
                "            fig, ax = plt.subplots(1, 2)\n",
                "            ax[0].imshow(f) \n",
                "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
                "            ax[0].axis(\"off\")\n",
                "\n",
                "            # Transform and plot image\n",
                "            # Note: permute() will change shape of image to suit matplotlib \n",
                "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
                "            transformed_image = transform(f).permute(1, 2, 0) \n",
                "            ax[1].imshow(transformed_image) \n",
                "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
                "            ax[1].axis(\"off\")\n",
                "            fig.suptitle(f\"Class: {Path(random_image_path).parent.stem}\", fontsize=16)\n",
                "\n",
                "plot_transformed_images(image_path_list, transform=data_transform, n=3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading the data\n",
                "\n",
                "So far, we created a data transformation function. We are ready to load our dataset using this function. The easiest way to load data is to use the ImageFolder function in PyTorch. Let's load the dataset with this function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchvision import datasets\n",
                "\n",
                "# Creating training set\n",
                "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
                "                                  transform=data_transform, # transforms to perform on data (images)\n",
                "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
                "#Creating test set\n",
                "test_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
                "\n",
                "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n",
                "\n",
                "# Get class names as a list\n",
                "class_names = train_data.classes\n",
                "print(\"Class names: \",class_names)\n",
                "\n",
                "# Can also get class names as a dict\n",
                "class_dict = train_data.class_to_idx\n",
                "print(\"Class names as a dict: \",class_dict)\n",
                "\n",
                "# Check the lengths\n",
                "print(\"The lengths of the training and test sets: \", len(train_data), len(test_data))\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "lets have a look at a single image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img, label = train_data[0][0], train_data[0][1]\n",
                "print(f\"Image tensor:\\n{img}\")\n",
                "print(f\"Image shape: {img.shape}\")\n",
                "print(f\"Image datatype: {img.dtype}\")\n",
                "print(f\"Image label: {label}\")\n",
                "print(f\"Label datatype: {type(label)}\")\n",
                "\n",
                "# Rearrange the order of dimensions\n",
                "img_permute = img.permute(1, 2, 0)\n",
                "\n",
                "# Print out different shapes (before and after permute)\n",
                "print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n",
                "print(f\"Image permute shape: {img_permute.shape} -> [height, width, color_channels]\")\n",
                "\n",
                "# Plot the image\n",
                "plt.figure(figsize=(10, 7))\n",
                "plt.imshow(img.permute(1, 2, 0))\n",
                "plt.axis(\"off\")\n",
                "plt.title(class_names[label], fontsize=14);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### Turn loaded images into DataLoader'sÂ¶\n",
                "\n",
                "So far, we loaded images. Note that a DataLoader in PyTorch is a utility used to load data from a dataset object in parallel. It allows the user to load data in batches, which can be useful for training deep learning models, as it enables the model to process multiple samples at once, which can speed up the training process. Additionally, it also allows the user to shuffle the data, which can help to prevent overfitting.\n",
                "\n",
                "The DataLoader takes a dataset object and several other optional parameters, such as the batch size, the number of worker threads to use for loading the data, and a boolean flag for whether or not to shuffle the data. The DataLoader will then return an iterator that can be used to iterate over the data in batches.\n",
                "\n",
                "Excellent! we talked a little bit about DataLoader. Show time:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader\n",
                "\n",
                "\n",
                "\n",
                "# Turn train and test Datasets into DataLoaders\n",
                "train_dataloader = DataLoader(dataset=train_data, \n",
                "                              batch_size=1, # how many samples per batch?\n",
                "                              shuffle=True) # shuffle the data?\n",
                "\n",
                "test_dataloader = DataLoader(dataset=test_data, \n",
                "                             batch_size=1, \n",
                "                             shuffle=False) # don't usually need to shuffle testing data\n",
                "\n",
                "train_dataloader, test_dataloader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img, label = next(iter(train_dataloader))\n",
                "\n",
                "# Note that batch size will now be 1.  \n",
                "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
                "print(f\"Label shape: {label.shape}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data augmentation\n",
                "\n",
                "\n",
                "Have you heard of what data augmentation is? Data augmentation is a technique used to artificially increase the size of a dataset by applying random modifications to the existing data. This can help to improve the performance of deep learning models by providing the model with more diverse training examples. Data augmentation can be useful when the available dataset is small or when the model is prone to overfitting.\n",
                "\n",
                "Some common data augmentation techniques include:\n",
                "\n",
                "    Random flipping or rotation of images\n",
                "    Random cropping of images\n",
                "    Random changes to brightness, contrast or color\n",
                "    Adding noise to images\n",
                "    Scaling or translation of images\n",
                "\n",
                "By using data augmentation techniques, the model can learn to generalize better and become more robust to small variations in the data. This can help to prevent overfitting and improve the model's performance on unseen data. It is important to note that data augmentation should be performed before data preprocessing. Keep in mind data augmentation should be applied only to the training set, not the validation or test set."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "#### Creating transforms and loading data\n",
                "\n",
                "TrivialAugmentWide is a data augmentation technique in PyTorch that applies random resizing and cropping to an image. The technique is intended to be used as a \"wide\" data augmentation technique, meaning that it makes a large number of random transformations to the image in order to increase the diversity of the training data. This can help to improve the robustness and generalization of a machine learning model. This example illustrates the various transforms available in the torchvision.transforms module.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set image size.\n",
                "IMAGE_WIDTH = 64\n",
                "IMAGE_HEIGHT = 64\n",
                "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
                "\n",
                "# Create training transform with TrivialAugment\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize(IMAGE_SIZE),\n",
                "    transforms.TrivialAugmentWide(),\n",
                "     transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(10),\n",
                "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
                "    transforms.ToTensor(),\n",
                "    ])\n",
                "\n",
                "# Create testing transform (no data augmentation)\n",
                "test_transform = transforms.Compose([\n",
                "    transforms.Resize(IMAGE_SIZE),\n",
                "    transforms.ToTensor()])\n",
                "\n",
                "\n",
                "# Turn image folders into Datasets\n",
                "train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform)\n",
                "test_data_augmented = datasets.ImageFolder(test_dir, transform=test_transform)\n",
                "\n",
                "# Set some parameters.\n",
                "BATCH_SIZE = 32\n",
                "torch.manual_seed(42)\n",
                "\n",
                "train_dataloader_augmented = DataLoader(train_data_augmented, \n",
                "                                        batch_size=BATCH_SIZE, \n",
                "                                        shuffle=True)\n",
                "\n",
                "test_dataloader_augmented = DataLoader(test_data_augmented, \n",
                "                                       batch_size=BATCH_SIZE, \n",
                "                                       shuffle=False,)\n",
                "\n",
                "train_dataloader_augmented, test_dataloader_augmented"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CNN image classifier \n",
                "\n",
                "\n",
                "A convolutional neural network (CNN) is a type of deep learning neural network that is primarily used for image and video recognition tasks. CNNs are designed to process data that has a grid-like topology, such as an image, which is composed of pixels arranged in a 2D grid. The architecture of a CNN consists of several layers, including convolutional layers, pooling layers, and fully connected layers.\n",
                "\n",
                "The convolutional layers are responsible for detecting features in the input image by applying a set of learnable filters to the image. These filters are convolved with the input image to produce a set of feature maps, which are then passed through pooling layers to reduce the spatial dimensions of the feature maps and retain only the most salient features. The fully connected layers then process the pooled feature maps to produce the final output, such as a classification label.\n",
                "\n",
                "CNNs have been used to achieve state-of-the-art results on a wide range of computer vision tasks such as image classification, object detection, and semantic segmentation. Now let's go ahead and build a CNN model with nn.Module in Pytorch.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from torch import nn \n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "device\n",
                "\n",
                "#  Improved CNN-based image classifier\n",
                "class ImageClassifier(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        \n",
                "        # First convolutional block with batch normalization and LeakyReLU\n",
                "        self.conv_layer_1 = nn.Sequential(\n",
                "            nn.Conv2d(3, 6, 3, padding=1),  # Moderate increase from 4 to 6\n",
                "            nn.BatchNorm2d(6),\n",
                "            nn.LeakyReLU(0.1),\n",
                "            nn.MaxPool2d(2)\n",
                "        )\n",
                "        \n",
                "        # Second convolutional block with batch normalization and LeakyReLU\n",
                "        self.conv_layer_2 = nn.Sequential(\n",
                "            nn.Conv2d(6, 8, 3, padding=1),  # Moderate increase from 4 to 8\n",
                "            nn.BatchNorm2d(8),\n",
                "            nn.LeakyReLU(0.1),\n",
                "            nn.MaxPool2d(2)\n",
                "        )\n",
                "        \n",
                "        # For a 64x64 input, after 2 MaxPool2d(2) layers, the spatial dimensions are 16x16\n",
                "        # With 8 channels, the flattened size is 16*16*8 = 2048\n",
                "        \n",
                "        # Classifier with dropout\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Flatten(),\n",
                "            nn.Dropout(0.25),  # Add dropout for regularization\n",
                "            nn.Linear(in_features=16*16*8, out_features=2)\n",
                "        )\n",
                "        \n",
                "        # For residual connection\n",
                "        self.downsample = nn.Sequential(\n",
                "            nn.Conv2d(3, 8, 1, stride=4),  # Match spatial dimensions (64x64 -> 16x16)\n",
                "            nn.BatchNorm2d(8)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Save input for residual connection\n",
                "        identity = self.downsample(x)\n",
                "        \n",
                "        x = self.conv_layer_1(x)\n",
                "        x = self.conv_layer_2(x)\n",
                "        \n",
                "        # Add residual connection\n",
                "        x = x + identity\n",
                "        \n",
                "        x = self.classifier(x)\n",
                "        \n",
                "        return x\n",
                "# Instantiate an object.\n",
                "model = ImageClassifier().to(device)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ok, we created a CNN-based model. But does this model work? To understand this, let's pass a image through the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Get a batch of images and labels from the DataLoader\n",
                "img_batch, label_batch = next(iter(train_dataloader_augmented))\n",
                "\n",
                "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
                "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
                "print(f\"Single image shape: {img_single.shape}\\n\")\n",
                "\n",
                "# 3. Perform a forward pass on a single image\n",
                "model.eval()\n",
                "with torch.inference_mode():\n",
                "    pred = model(img_single.to(device))\n",
                "    \n",
                "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
                "print(f\"Output logits:\\n{pred}\\n\")\n",
                "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
                "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
                "print(f\"Actual label:\\n{label_single}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It is important to understand the model architecture. Fortunately, there is the torchinfo package to see the architecture of the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install torchinfo if it's not available, import it if it is\n",
                "try: \n",
                "    import torchinfo\n",
                "except:\n",
                "    !pip install torchinfo\n",
                "    import torchinfo\n",
                "    \n",
                "from torchinfo import summary\n",
                "# do a test pass through of an example input size \n",
                "summary(model, input_size=[1, 3, IMAGE_WIDTH ,IMAGE_HEIGHT]) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### Create train & test loop functions\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_step(model: torch.nn.Module, \n",
                "               dataloader: torch.utils.data.DataLoader, \n",
                "               loss_fn: torch.nn.Module, \n",
                "               optimizer: torch.optim.Optimizer):\n",
                "    # Put model in train mode\n",
                "    model.train()\n",
                "    \n",
                "    # Setup train loss and train accuracy values\n",
                "    train_loss, train_acc = 0, 0\n",
                "    \n",
                "    # Loop through data loader data batches\n",
                "    for batch, (X, y) in enumerate(dataloader):\n",
                "        # Send data to target device\n",
                "        X, y = X.to(device), y.to(device)\n",
                "        \n",
                "        # 1. Forward pass\n",
                "        y_pred = model(X)\n",
                "\n",
                "        # 2. Calculate  and accumulate loss\n",
                "        loss = loss_fn(y_pred, y)\n",
                "        train_loss += loss.item() \n",
                "\n",
                "        # 3. Optimizer zero grad\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        # 4. Loss backward\n",
                "        loss.backward()\n",
                "\n",
                "        # 5. Optimizer step\n",
                "        optimizer.step()\n",
                "\n",
                "        # Calculate and accumulate accuracy metric across all batches\n",
                "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
                "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
                "\n",
                "    # Adjust metrics to get average loss and accuracy per batch \n",
                "    train_loss = train_loss / len(dataloader)\n",
                "    train_acc = train_acc / len(dataloader)\n",
                "    return train_loss, train_acc\n",
                "\n",
                "\n",
                "def test_step(model: torch.nn.Module, \n",
                "              dataloader: torch.utils.data.DataLoader, \n",
                "              loss_fn: torch.nn.Module):\n",
                "    # Put model in eval mode\n",
                "    model.eval() \n",
                "    \n",
                "    # Setup test loss and test accuracy values\n",
                "    test_loss, test_acc = 0, 0\n",
                "    \n",
                "    # Turn on inference context manager\n",
                "    with torch.inference_mode():\n",
                "        # Loop through DataLoader batches\n",
                "        for batch, (X, y) in enumerate(dataloader):\n",
                "            # Send data to target device\n",
                "            X, y = X.to(device), y.to(device)\n",
                "    \n",
                "            # 1. Forward pass\n",
                "            test_pred_logits = model(X)\n",
                "\n",
                "            # 2. Calculate and accumulate loss\n",
                "            loss = loss_fn(test_pred_logits, y)\n",
                "            test_loss += loss.item()\n",
                "            \n",
                "            # Calculate and accumulate accuracy\n",
                "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
                "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
                "            \n",
                "    # Adjust metrics to get average loss and accuracy per batch \n",
                "    test_loss = test_loss / len(dataloader)\n",
                "    test_acc = test_acc / len(dataloader)\n",
                "    return test_loss, test_acc\n",
                "\n",
                "\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# 1. Take in various parameters required for training and test steps\n",
                "def train(model: torch.nn.Module, \n",
                "          train_dataloader: torch.utils.data.DataLoader, \n",
                "          test_dataloader: torch.utils.data.DataLoader, \n",
                "          optimizer: torch.optim.Optimizer,\n",
                "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
                "          epochs: int = 5):\n",
                "    \n",
                "    # 2. Create empty results dictionary\n",
                "    results = {\"train_loss\": [],\n",
                "        \"train_acc\": [],\n",
                "        \"test_loss\": [],\n",
                "        \"test_acc\": []\n",
                "    }\n",
                "    \n",
                "    # 3. Loop through training and testing steps for a number of epochs\n",
                "    for epoch in tqdm(range(epochs)):\n",
                "        train_loss, train_acc = train_step(model=model,\n",
                "                                           dataloader=train_dataloader,\n",
                "                                           loss_fn=loss_fn,\n",
                "                                           optimizer=optimizer)\n",
                "        test_loss, test_acc = test_step(model=model,\n",
                "            dataloader=test_dataloader,\n",
                "            loss_fn=loss_fn)\n",
                "        \n",
                "        # 4. Print out what's happening\n",
                "        print(\n",
                "            f\"Epoch: {epoch+1} | \"\n",
                "            f\"train_loss: {train_loss:.4f} | \"\n",
                "            f\"train_acc: {train_acc:.4f} | \"\n",
                "            f\"test_loss: {test_loss:.4f} | \"\n",
                "            f\"test_acc: {test_acc:.4f}\"\n",
                "        )\n",
                "\n",
                "        # 5. Update results dictionary\n",
                "        results[\"train_loss\"].append(train_loss)\n",
                "        results[\"train_acc\"].append(train_acc)\n",
                "        results[\"test_loss\"].append(test_loss)\n",
                "        results[\"test_acc\"].append(test_acc)\n",
                "\n",
                "    # 6. Return the filled results at the end of the epochs\n",
                "    return results\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set random seeds\n",
                "torch.manual_seed(42) \n",
                "torch.cuda.manual_seed(42)\n",
                "\n",
                "# Set number of epochs (change to 1000 for better results)\n",
                "NUM_EPOCHS = 25\n",
                "# NUM_EPOCHS = 1000\n",
                "\n",
                "\n",
                "# Setup loss function and optimizer\n",
                "loss_fn = nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
                "\n",
                "# Start the timer\n",
                "from timeit import default_timer as timer\n",
                "start_time = timer()\n",
                "\n",
                "# Train model_0\n",
                "model_results = train(model=model,\n",
                "                      train_dataloader=train_dataloader_augmented,\n",
                "                      test_dataloader=test_dataloader_augmented,\n",
                "                      optimizer=optimizer,\n",
                "                      loss_fn=loss_fn,\n",
                "                      epochs=NUM_EPOCHS)\n",
                "\n",
                "# End the timer and print out how long it took\n",
                "end_time = timer()\n",
                "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To understand the performance of model, let's visualize the loss and accuracy values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 94,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_loss_curves(results):\n",
                "  \n",
                "    results = dict(list(model_results.items()))\n",
                "\n",
                "    # Get the loss values of the results dictionary (training and test)\n",
                "    loss = results['train_loss']\n",
                "    test_loss = results['test_loss']\n",
                "\n",
                "    # Get the accuracy values of the results dictionary (training and test)\n",
                "    accuracy = results['train_acc']\n",
                "    test_accuracy = results['test_acc']\n",
                "\n",
                "    # Figure out how many epochs there were\n",
                "    epochs = range(len(results['train_loss']))\n",
                "\n",
                "    # Setup a plot \n",
                "    plt.figure(figsize=(15, 7))\n",
                "\n",
                "    # Plot loss\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.plot(epochs, loss, label='train_loss')\n",
                "    plt.plot(epochs, test_loss, label='test_loss')\n",
                "    plt.title('Loss')\n",
                "    plt.xlabel('Epochs')\n",
                "    plt.legend()\n",
                "\n",
                "    # Plot accuracy\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
                "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
                "    plt.title('Accuracy')\n",
                "    plt.xlabel('Epochs')\n",
                "    plt.legend();\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_loss_curves(model_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Making predictions\n",
                "\n",
                "Now we had a good model for image classification. But, how does this model predict new data? To understand this let me make a prediction on a custom image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose a image.\n",
                "custom_image_path =  CATDOG_DATA_DIR + \"/CATDOG/test_set/test_set/dogs/dog.4001.jpg\"\n",
                "\n",
                "import torchvision\n",
                "# Load in custom image and convert the tensor values to float32\n",
                "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
                "\n",
                "# Divide the image pixel values by 255 to get them between [0, 1]\n",
                "custom_image = custom_image / 255. \n",
                "\n",
                "# Print out image data\n",
                "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
                "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
                "print(f\"Custom image dtype: {custom_image.dtype}\")\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### EZKL Stuff"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "x = 0.1*torch.rand(1,*[3, IMAGE_WIDTH ,IMAGE_HEIGHT], requires_grad=True)\n",
                "\n",
                "# Flips the neural net into inference mode\n",
                "model.eval()\n",
                "\n",
                "    # Export the model\n",
                "torch.onnx.export(model,               # model being run\n",
                "                      x,                   # model input (or a tuple for multiple inputs)\n",
                "                      \"network.onnx\",            # where to save the model (can be a file or file-like object)\n",
                "                      export_params=True,        # store the trained parameter weights inside the model file\n",
                "                      opset_version=10,          # the ONNX version to export the model to\n",
                "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
                "                      input_names = ['input'],   # the model's input names\n",
                "                      output_names = ['output'], # the model's output names\n",
                "                      dynamic_axes={'input' : {0 : 'batch_size'},\n",
                "                                    'input.1' : {0 : 'batch_size'}, # variable length axes\n",
                "                                    'output' : {0 : 'batch_size'}})\n",
                "\n",
                "data_array = ((x).detach().numpy()).reshape([-1]).tolist()\n",
                "\n",
                "data = dict(input_data = [data_array])\n",
                "\n",
                "    # Serialize data into file:\n",
                "json.dump( data, open(\"input.json\", 'w' ))\n",
                "\n",
                "\n",
                "# ezkl.export(circuit, input_shape = [[1], [1025, 130]], run_gen_witness=False, run_calibrate_settings=False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we set the visibility of the different parts of the circuit, whereby the model params and the outputs of the computational graph (the key and the judgment) are public"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 98,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ezkl\n",
                "import os \n",
                "\n",
                "\n",
                "\n",
                "run_args = ezkl.PyRunArgs()\n",
                "run_args.input_visibility = \"private\"\n",
                "run_args.param_visibility = \"fixed\"\n",
                "run_args.output_visibility = \"public\"\n",
                "run_args.variables = [(\"batch_size\", 1)]\n",
                "\n",
                "\n",
                "# TODO: Dictionary outputs\n",
                "res = ezkl.gen_settings(py_run_args=run_args)\n",
                "assert res == True\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we generate a settings file. This file basically instantiates a bunch of parameters that determine their circuit shape, size etc... Because of the way we represent nonlinearities in the circuit (using Halo2's [lookup tables](https://zcash.github.io/halo2/design/proving-system/lookup.html)), it is often best to _calibrate_ this settings file as some data can fall out of range of these lookups.\n",
                "\n",
                "You can pass a dataset for calibration that will be representative of real inputs you might find if and when you deploy the prover. Here we use the validation dataset we used during training. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "res = await ezkl.calibrate_settings(\"input.json\", target=\"resources\", scales = [4])\n",
                "assert res == True\n",
                "print(\"verified\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "res = ezkl.compile_circuit()\n",
                "assert res == True"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we use Halo2 with KZG-commitments we need an SRS string from (preferably) a multi-party trusted setup ceremony. For an overview of the procedures for such a ceremony check out [this page](https://blog.ethereum.org/2023/01/16/announcing-kzg-ceremony). The `get_srs` command retrieves a correctly sized SRS given the calibrated settings file from [here](https://github.com/han0110/halo2-kzg-srs). \n",
                "\n",
                "These SRS were generated with [this](https://github.com/privacy-scaling-explorations/perpetualpowersoftau) ceremony. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "res = await ezkl.get_srs()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now need to generate the (partial) circuit witness. These are the model outputs (and any hashes) that are generated when feeding the previously generated `input.json` through the circuit / model. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "res = await ezkl.gen_witness()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As a sanity check we can run a mock proof. This just checks that all the constraints are valid. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "res = ezkl.mock()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we setup verifying and proving keys for the circuit. As the name suggests the proving key is needed for ... proving and the verifying key is needed for ... verifying. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!export RUST_LOG=trace\n",
                "# HERE WE SETUP THE CIRCUIT PARAMS\n",
                "# WE GOT KEYS\n",
                "# WE GOT CIRCUIT PARAMETERS\n",
                "# EVERYTHING ANYONE HAS EVER NEEDED FOR ZK\n",
                "res = ezkl.setup()\n",
                "\n",
                "assert res == True"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we generate a full proof. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GENERATE A PROOF\n",
                "\n",
                "\n",
                "res = ezkl.prove(proof_path=\"proof.json\")\n",
                "\n",
                "print(res)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And verify it as a sanity check. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# VERIFY IT\n",
                "\n",
                "res = ezkl.verify()\n",
                "\n",
                "assert res == True\n",
                "print(\"verified\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "res = await ezkl.create_evm_verifier()\n",
                "\n",
                "assert res == True"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Verify if the Verifier Works Locally"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Deploy The Contract"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make sure anvil is running locally first\n",
                "# run with $ anvil -p 3030\n",
                "# we use the default anvil node here\n",
                "import json\n",
                "\n",
                "address_path = os.path.join(\"address.json\")\n",
                "\n",
                "res = await ezkl.deploy_evm(\n",
                "    address_path,\n",
                "    rpc_url='http://127.0.0.1:3030'\n",
                ")\n",
                "\n",
                "assert res == True\n",
                "\n",
                "with open(address_path, 'r') as file:\n",
                "    addr = file.read().rstrip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# make sure anvil is running locally\n",
                "# $ anvil -p 3030\n",
                "\n",
                "res = await ezkl.verify_evm(\n",
                "    addr,\n",
                "    rpc_url=\"http://127.0.0.1:3030\"\n",
                ")\n",
                "assert res == True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
